---
layout: post
title: 近段时间总结
---

在没有对经验风险最小化，结构风险最小化，用训练误差（即经验风险）去逼近期望风险，泛化误差上界和VC维等等进行仔细学习之前，完全没有料到原来SVM就是建立在结构风险最小化和VC维理论上的一套算法。SVM很优美，曾经一度是机器学习领域里的王者。主要原因便是它的VC维很低（同样比较低的是LDA），而VC维比较低的算法一般来说都具有简单优美的特性，比如SVM需要的训练样本数量非常小（只需要支持向量附近的那些点就足够了）。


具体的关于用经验风险去逼近期望风险的理论和VC维理论在Andrew NG斯坦福公开课笔记的第六部分《Learning Theory》，全部推导以Hoeffding 不等式为起点，先是得到期望风险以一定的概率落入经验风险的一定的区间里，进而得到算法的样本复杂度（sample complexity）m，这个m便是算法在进行训练的时候需要多少的样本容量来支撑起算法的精度要求，如果样本容量太小，则得到的模型有可能精度太低（也就是经验风险和期望风险相差太大），只有在样本容量达到一定值之后，算法的精度才能满足我们的要求。而m除了与我们认为设定的概率值和上面两个风险相差的置信度相关外，只与假设空间的大小有关。


紧接着，引出VC维的概念，对于N个样本点，有个标记方式，若函数集H中存在假设总能将这些标记方式分开，则称为函数集H能打散这N个样本点，而H能打散的最大的N，称为H的VC维，有结论：n维空间里的线性决策面的VC维是n+1.这样便将假设空间的大小转化到VC维的讨论上来了。


VC维大致与模型参数个数线性相关，模型参数越多，模型越复杂，VC维越大。因此VC维是模型复杂度的表征量。


需要的训练样本容量与VC维线性相关，故也与模型参数个数线性相关。这样，因为SVM的VC维较低，因此需要的训练样本较小。


模型参数的个数也就是特征维度。