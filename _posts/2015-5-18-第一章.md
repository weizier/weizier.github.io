---
layout: post
title: 统计学习方法概论
---

统计学习关于数据的基本假设是同类数据具有一定的统计规律。

统计学习由监督学习、半监督学习、监督学习、强化学习等组成。在监督学习的背景下，统计学习方法主要包括模型的假设空间、模型筛选的准则和模型学习的算法。这分别对应着模型、策略和算法，被称作监督学习的三要素。

实现统计学习方法的一般步骤如下：
> 获取训练数据样本
> 确定可能模型的假设空间
> 确定模型选择的准则，一般称为损失函数
> 实现求解最优模型的算法
> 训练
> 利用得到的模型进行预测

统计学习方法的应用领域至少包含以下这些方面：人工智能(Artificial Intelligence)、模式识别(Pattern Recognition)、数据挖掘(Data Mining)、自然语言处理(Natural Language Processing)、语音识别(Speech Recognition)、图像识别(Image Recognition)、信息检索(Information Retrieval)和生物信息(Bioinformatics)等。

###监督模型的输入空间、特征空间和输出空间。
监督学习中，将输入和输出所有可能取值的集合分别称为输入空间和输出空间。输入空间可以和输出空间为同一个空间，也可以不同，但通常来说，输出空间都要远小于输入空间。
每个具体的输入是一个实例，通常由特征向量表示，所有特征向量存在的空间称为特征空间。特征空间的每一维对应一个特征。特征空间和输入空间有时候为同一个空间，对二者不予区分，有时候二者不一样，特征空间是从输入空间进行映射后得到的。所以在这个时候输入空间和特征空间就不一样了。要注意的是，模型都是定义在特征空间上的，也就是说模型最直接的输入是这些特征，而非最开始的输入。
假如训练集有N个样本点，每一个样本点都可以这样表示：(x下标i,y下标i)，而x下标i是有n个特征的。
> 回归问题：输入和输出都是连续
> 分类问题：输入连续，输出有限离散
> 标记问题：输入和输出都是离散序列

###联合概率分布
监督学习假设输入和输出的随机变量X和Y遵循联合概率分布P(X,Y)，这是监督学习的基本假设。在学习的过程中假定这一联合概率分布存在，但其具体形式并未明确。并且依照模型实际应用情况可能需要联合概率分布的参与模型训练（生成模型），也有可能不需要联合概率显式的参与模型训练过程（判别模型）（但并不意味着不存在，反而其存在性是模型训练的基本前提的）。
模型属于由输入空间到输出空间映射的集合，这个集合就是假设空间(Hypothesis space)，监督学习的模型分为两种：概率模型和非概率模型，前者的模型由P(Y|X)，如朴素贝叶斯；非概率模型由决策函数Y=f(X)来表示，如感知机，LR,SVM等等.


##统计学习三要素
###模型
监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。
###策略
损失函数度量模型一次预测的好坏，风险函数度量模型平均意义下的预测好坏。
常见的损失函数包括：0-1损失函数，平方损失函数，绝对值损失函数，对数损失函数，指数损失函数等等。
风险函数：模型f(X)关于联合概率分布P(X,Y)的平均损失，而经验风险：模型f(X)关于训练数据集的平均损失。我们的任务就是尽量用经验风险去逼近风险函数。为什么要逼近，因为风险函数是无法得到的，甚至永远都无法得到。或者说假如知道了联合概率分布P(X,Y)那么通过相关推导便可到我们想要的P(Y|X)，那便不需要学习了。因此，只能利用经验风险去逼近风险函数（或者叫期望风险）。根据大数定律，在N趋于无穷大时，经验风险趋于期望风险，这有些类似于频率和概率的区别，我们在求解一个事件的概率的时候，也往往都是利用频率趋于概率这个特性来求得的，此处照样如此。

####大数定律和中心极限定律的区别和联系
在无数次独立同分布的随机事件中，事件的频率趋于一个稳定的概率值，这是大数定律；
而同样的无数次独立同分布的随机事件中，事件的分布趋近于一个稳定的正态分布，而这个正太分布的期望值u，正是大数定律里面的概率值，这是中心极限定理所描述的。
所以，中心极限定理比大数定律揭示的现象更深刻，同时成立的条件当然也要相对来说苛刻一些。

现实应用中，由于训练数据往往有限，经验风险去估计期望风险往往不理想，需要对经验风险进行一些矫正，关系到两个策略：经验风险最小化和结构风险最小化。
####经验风险最小化
当模型是条件概率模型，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。
当样本容量很小时，会出现过拟合的现象。为了防止过拟合的产生，提出了结构风险最小化的策略，其实就等价于正则化。在经验风险最小化的基础之上，加上表征模型复杂度的正则化项。正则化项有多种形式，比如L1范数和L2范数。
当模型是条件概率模型，损失函数是对数损失函数，模型复杂度用模型的先验概率表示，此时极大后验概率便等价于结构风险最小化。

####频率学派和贝叶斯学派
二者的根本分歧在于对模型参数的分布观点不同，频率学派认为模型参数是固定的，只是未可知；贝叶斯学派认为模型的参数是有一定分布的，并不是固定的一个常数。

###算法
在算法这个步骤，有了待搜索的模型假设空间，也有表征模型好坏的损失函数，统计学习问题便归结为最优化问题，统计学习算法便成为求解最优化问题的算法。

##模型的选择
模型的选择主要方法有正则化和交叉验证，正则化项里边有个L1范数和L2范数的区别，实际还有个L0范数，L0范数和L1范数类似，都是会筛选特征的，但是L1在求解的时候可以相对较方便的求得，所以一般使用L1范数。
####交叉验证
如果样本数据特别充分，最简单的交叉验证是随机的将数据样本分成三个集合：训练集，验证集和测试集。训练集用来训练模型，验证集用来选择模型，而测试集用来最终对模型的评估。交叉验证分为：简单交叉验证，S折交叉验证和留一交叉验证

生成模型：学习联合概率分布，并通过该联合概率分布的进一步推导从而得到P(Y|X)
判别模型：仅对P(Y|X)进行计算，中间不通过联合概率的分布P(X,Y)，而是直接得到的。

