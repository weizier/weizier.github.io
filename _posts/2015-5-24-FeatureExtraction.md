---
layout: post
title: Feature Extraction
---

Feature extraction和feature selection 都同属于Dimension reduction,和feature selection不同之处在于feature extraction是在原有特征基础之上去创造凝练出一些新的特征出来，但是feature selection则只是在原有特征上进行筛选。Feature extraction有多种方法，包括PCA,LDA,LSA等等，相关算法则更多，pLSA,LDA,ICA,FA,UV-Decomposition,LFM,SVD等等。这里面有一个共同的算法，那就是鼎鼎大名的SVD。

一，SVD
SVD本质上是一种数学的方法， 它并不是一种什么机器学习算法，但是它在机器学习领域里有非常广泛的应用。

1.1 特征值分解
在讲SVD之前再讲讲特征值分解（eigenvalue decomposition），别看名字很高大上，实际非常简单。首先，对于一个方阵A，我们可以求得它的特征值和特征向量。

矩阵的本质其实就是变换，比如线性空间里的矩阵对应线性变换。而特征向量就描述了这些变换的方向，特征值描述的是变换的幅度。选取特征值比较大的几组特征向量，那么也就描述了最大程度上对原变换的近似。
由于特征值和特征向量对不止一个，也许有很多组。那么，我们可以得到A的特征值分解如下：

其中，Q是A的特征向量组成的矩阵，D是特征值组成的对角矩阵。特征值分解有一个最大的缺点，那就是只能应用在方阵分解的情况下，这大大限制了它的应用。为了解决这一点，提出奇异值分解。

    1.2 奇异值分解
SVD是一种矩阵分解方法，将一个矩阵分解成三个矩阵相乘。和特征值分解不一样，奇异值分解可以针对任何类型的矩阵进行准确的分解。

可以解释如下：


所以，U是的特征向量，而是的特征值。同理：


所以同样的，V是的特征向量，而是的特征值。注意：是对角矩阵，U和V都是单位正交矩阵。是矩阵M的奇异值，也就是或者特征值的平方根。U被称作左奇异矩阵，V被称为右奇异矩阵。一般来说，分解成功后的有很多非零值，但这些非零值并不全是我们想要的，于是我们把其中一些比较下的非零值去除，然后对U的相应列和V的相应行去除，从而将矩阵进行了简化，当然这个时候得到的M矩阵和原始矩阵并不完全相等，但只要误差在允许范围内就行，并且我们最主要的目的就是为了降维。

二，PCA
PCA的目标是在新的低维空间上有最大的方差，也就是原始数据在主成分上的投影要有最大的方差。这个是方差的解释法，而这正好对应着特征值最大的那些主成分。
有人说，PCA本质上是去中心化的SVD，这可以看出PCA内在上与SVD的联系。PCA的得到是先将原始数据X的每一个样本，都减去所有样本的平均值，然后再用每一维的标准差进行归一化。假如原始矩阵X的每一行对应着每一个样本，列对应着相应的特征，那么上述去中心化的步骤对应着先所有行求平均值，得到的是一个向量，然后再将每一行减去这个向量，接着，针对每一列求标准差，然后再把每一列的数据除以这个标准差。这样得到的便是去中心化的矩阵了。
紧接着，求它的协方差矩阵，因为已经去中心化了， 所以协方差矩阵就可以用代替，求得协方差矩阵的特征值和特征向量。然后将特征值从大到小排序，选取最大的k个特征和相应的特征向量。这些被选取的特征向量就叫做主成分（Component factor）。
实际上，PCA的主成分获取和SVD有非常大的相似之处，我们上面讲到SVD的左奇异矩阵和右奇异矩阵分别对应着和，而PCA的主成分是选取的的最大k个特征向量，所以PCA在这一点上和SVD非常类似。
顺便提提FA,ICA,PCA的区别:
1.FA是挖掘数据背后的隐藏结构，认为原始数据都是k维空间里隐含变量产生的，于是将每一个原始数据都分解成这些隐含变量的线性组合。FA认为高维样本点都是通过低维样本点通过高斯分布，线性变换，误差扰动而产生，所以FA主要分析数据的相关性；
2.ICA主要是在盲信号分析领域里非常有效，认为样本数据都是由非高斯分布的隐含因子产生，用来还原混合信号里的原始信号。其最重要的假设是：非高斯分布和统计独立。
3.PCA是归纳总结，将原始数据总结到一个新的低维空间中去，k维空间里的新数据都是原始数据的线性组合，从这一点可以看出PCA和FA对待原始数据的处理方向并不一致。
说到PCA的新数据是从原始数据归纳总结中得到的，也就是新数据都是原始数据的线性组合。一定要注意，这里并不是说主成分。主成分仍然是对应特征值最大的那k个，而新数据正是通过这k个互相正交的主成分线性变换得到的。详情一定要见Andrew NG的讲义。
并且，PCA的实现一般有两种，一种是用特征值分解去实现的，一种是用奇异值分解去实现的。PCA所用的jordan分解等于是两个svd分解相乘。其实两者数学本质完全相同，现在一般的pca都是用svd来实现的。关于PCA用SVD来实现，相应论文里有提到。
另外，由于PCA是对原始数据的线性变换，针对非线性变换的主成分分析有Kernel PCA.
m个样本组成一个原始数据的矩阵，每一个样本的特征都是n维，也就是说原始数据矩阵是n×m的。

1.去中心化
    也就是将每一个样本先减去所有样本的平均值，然后再求每一行的标准差，并将每一行的数据都除以该标准差。这是对每一维特征进行归一化，以便更好的筛选出那些在主成分方向上有最大的方差，只有用各自标准差归一化之后每一维特征才具有可比性。
2.SVD分解
去中心化后的矩阵X进行SVD分解，实际上是求的特征值和特征向量，并根据所求的特征值从大到小筛选出最大的k个，并将对应的特征向量组成新的矩阵U。右奇异矩阵同样如此，在PCA的实际应用过程中，并不需要算出V，这里只是为了更加凸显出PCA和SVD的关系。

这个时候，注意并不是矩阵X的精确SVD，因为省略掉了一些奇异值比较小的特征向量。我们将X进行如下操作，看看发生了什么：


看到没有，这就是PCA啊，这就是将原来n维的原始数据降维到了k维空间中。并且新数据都是原始数据的线性组合。注意，对原始数据右乘V，并不能将原始数据映射到新的k维空间中去。在X这样的表示形式下（指X由样本的列向量组成），就只能用的特征向量进行PCA。你要把原始数据表示成X的转置这样的形式，那也可以，但是相应的特征向量就是的特征向量，所以还是一回事。为什么，用X右乘V就知道了。（变成了n×m矩阵和n×m矩阵相乘，这样的矩阵如何相乘？所以只能把X转置，但是一转置的话，V矩阵就变成了上面的U矩阵了。）




三．LSA(Latent Semantic Analysis)
LSA在信息检索领域里也被称作Latent Semantic Indexing(LSI).用来分析文章词背后的隐含语义的，它的一大假设便是把文章当做“bag of words”，也就是说，文档里词的顺序并不重要，而是词的数量主导。
LSA是自然语言处理里面的一项技术，通过衍生出一系列概念来分析文档和词的联系。这些概念便是隐含在文档和词之间的抽象成分。文档先是由Vector Space Model 来表征，每一行对应一个词，每一列对应一个文档，矩阵里的数表示可以用词频来表示，其实大多都是用TF.IDF来表征。并且应用SVD来减少维数同时又要保证保留大部分的信息不被丢失。
用SVD降维有时候可以理解成将几个词总结合并成一个词，比如
{(car), (truck), (flower)} --> {(1.3452 * car + 0.2828 * truck), (flower)}
    实际上，这就是一种概念的抽象。
    将原始数据表示成矩阵X，每一行代表一个词，每一列代表一篇文档。

那么就是计算两个词在文档水平上的相似性，则计算的便是所有词在文档上的相似性，并且该矩阵对称。同样的，得到的是文档基于词上的相似性，是所有文档在词上的相似性。注意，这里计算相似性都是限定在这些文档或文档所包含词的范围内讨论。下面很好的分析了如何用SVD将这个向量空间模型分解成三个矩阵相乘。其实就是上面讨论的SVD。





我的理解是：LSA实际上就是SVD在自然语言处理领域里的应用，根本谈不上一个全新的算法。LSA有如下缺点：
1.无法处理多义词，由于LSA基于统计，每个词都认为只有一个意思，不管这个词出现在什么语境，都被当做是同一个意思。比如在一篇金融文档和一篇旅游文档里，都含有bank这个词，那么在计算文档相似性的时候，bank这个词会被认为是同一个意思，从而对两篇文档的相似性有一定的误导作用。实际这两篇文档很可能毫不相干。
2.前面说过，LSA的词-文档矩阵就是一个词袋模型，词袋模型的最大缺点就是丢失了词序这个非常重要的特征。这是很多用词袋模型来表征文档的其他许多算法的共同缺点。
3.LSA假定词和文档服从联合高斯分布，这在实际当中可能不对，比如有些是泊松分布，pLSA便是针对此问题被提出来的， 比传统的LSA效果要好。
4.LSA根本就不能解决一词多义的问题，也就是多义词。恰恰相反， 同义词正是LSA的缺点之一。倒是一义多词有可能可以通过LSA解决，也就是根据隐含主题去给各个词聚类，经常一起出现在同一个隐含主题下的词，更有可能是同义词。
5.这个不能算作LSA的缺点，LSA映射到低维的这个k值如何选择？这是一个值得思考的问题。

这里补充一个非常重要的关于LSA和pLSA的区别：
联系：pLSA是LSA对应的概率模型，二者都是非监督的。
区别：1. LSA纯粹基于数学运算上的SVD，也就是它的背景只有线性代数。而pLSA则有一个非常强的概率背景（隐变量模型Latent Variable Model）;
      2. 在用SVD做k阶近似的时候（也就是选取最大的k个特征值对应的那些特征向量做SVD近似），使用的损失函数是平方损失函数（实际就是Frobenius norm，很多情况下用到SVD的算法都是用的这个），也就是最小二乘法，而在机器学习领域里，最小二乘法就是数据噪声服从高斯分布下的极大似然估计求解。因此LSA是假定了词频噪声服从高斯分布的，这有时候不符合实际情况，因此pLSA对此加以改进；      3. 二者都能够处理近义词，但是对于LSA来说，它不能处理多义词，因为多义词在这里只有一个维度，即出现次数。而出现次数并不足以表征多义词；
      4. 二者都是对文集进行低维空间转换，当加入一篇新文档后，为了将训练出来的参数包含进这篇新文档的影响，二者都要重新运行一次算法，但是LSA只需要做矩阵分解，但是pLSA则需要将EM算法重新迭代。
      5. LSA在矩阵分解的时候有可能出现负数情况，但是pLSA则不会，其模型本身就限制了必须非负。根据SVD分解得到的原始矩阵近似有可能出现负数，这显然不符合实际情况。
      6.pLSA的对称模型，如下公式的前半部分，

这实际就是SVD，P(d|c)对应着文档-隐含主题矩阵，P(w|c)对应着词—隐含主题矩阵，P(c)则是隐含主题矩阵，也就是中间的奇异值矩阵，那么既然本质可以和SVD想通，为什么pLSA要比LSA效果好呢？主要原因就在于pLSA的这些概率参数的得到都是通过假定符合多项式分布，然后用EM算法求得的，背后有一个概率论作为支撑，而LSA则任何理论支撑都没有，唯一有的就是纯粹数学运算得到这些概率参数。


四．UV-Decomposition 和 LFM(Latent Factor Model)和NMF(Non-negative Matrix Factorization)
其实，这两种算法在我看来本质上一回事。唯一的区别在于前者用的是RMSE作为损失函数，而LFM使用平方损失函数，也就是最小二乘法。当然，为了避免过拟合，可以在二者的原始损失函数上添加一些矩阵的二范数，因为这里要分解成两个矩阵，所以就有两个矩阵的二范数。
有人说，LFM实际就是伪SVD，我认为是对的。LFM将矩阵分解成两个矩阵相乘，而SVD将矩阵分解成三个矩阵相乘，利用的损失函数都是平方损失函数，并且都有可能导致分解的矩阵中含有负数项，这在有些实际应用领域是不合逻辑的。
在看维基中关于NMF的介绍后，得到的直观感受是：1.NMF和前述二者似乎没有本质不同，只是添加了一个包括原始矩阵在内的三个矩阵都非负这个限制条件。2.NMF很强大，和很多算法都想通，比如pLSA，聚类等等。



五．Kullback–Leibler divergenc
简称为KL散度，是测量两个概率分布P和Q不匹配程度的数学量。Q针对P的散度，用DKL(P‖Q)来表示，这也可以理解为：当用Q去近似P的时候，信息丢失量的一个测量。一般来说，P代表真实值，而Q表示P的近似值，然后用DKL(P‖Q)来衡量这二者之间的差距。计算公式如下：

我们说的信息增益可以用KL散度来表示。如下
因此，首先我们可以将信息增益理解为X和Y的互信息（互信息的定义就是最后一个等式），同时，我们又可以将其理解为Q=P(X)P(Y)在P=P(X,Y)下的散度，表示为如下：




在用SVD去解释PCA的过程中，刚刚体会到了一种前所未有的洞察世界本质的感觉，真的就是一种豁然开朗，醍醐灌顶的快感。
我们的学习是什么，学习的本质是什么？其实在我看来就是一种特征抽取的过程，在学习一门新知识的时候，这里一个知识点，那儿一个知识点，你头脑里一篇混乱，完全不知所云，这些知识点在你的大脑中也纯粹是杂乱无章毫无头绪的，这不正是高维空间里数据的特征么？最本质的数据完全湮没在太多太多的扰动中，而我们要做的就是提炼，从一堆毫无头绪的扰动中寻找到最本质的真理。
经过一段时间的摸索，你上升到了一个台阶，从这个台阶上去看原来所学到的知识点，你突然之间豁然开朗，原来TMD这些概念，这些知识点都TM是想通的。为什么你原来却从这些知识点中看不到任何联系呢?原因就在于你之前的维度太高，而你永远只能在这个杂乱无章的高维空间里窥探到真理的一些细枝末叶，本来在低维空间里相互联系的事物，由于你看到的是这些事物在各个方向各个领域里的一部分投影，你所学到的只是这些投影，那你如何仅仅依靠这些少量的投影以管窥豹呢？不可能的，所以你的知识只能是杂乱无章，毫无头绪的。但是，一旦你所拥有的投影越来越多，你所学到的知识点越来越多，你就逐渐拥有了依靠投影获取全貌的能力，这个时候你会发现，哇，原来过去的那些都是想通的。这就是高维空间里杂乱无章的知识点，经过降维后，回归到了最本质特征的全过程。
从今以后，你可以只拿着这个低维空间里的真理，摒弃掉以前学习到的任何细枝末叶的东西，然后在任何需要的时候，经过这个降维的逆算法去还原到你所学到的知识点。
那么，人与人之间的区别在哪里呢？那就是，对任何一个新领域的知识点建立一套降维工具的能力。
反观SVD，PCA，LSA等等，它们做的不正是这些么？比如在文本分类领域，最初始的数据是将文档表示成向量空间模型的一个矩阵，而这个矩阵所拥有的就是不同的词，这里一个词，那里一个词，对于我们人类来说，我们都已经拥有将不同词在低维空间上总结归纳的能力，知道这些词的联系和区别，但是对于计算机来说，它们怎么知道这些的联系呢？也就是它们根本还不拥有这些降维的能力，那么就要依靠我们人类告诉它们这个方法，这个工具就是SVD，其核心思想就是：将这些不同的词都映射到低维空间中去，在低维空间中去总结，去发现这些词的内在联系，一旦这些内在联系建立了，那么我们就知道了这些文档的内在联系了。这不正是高维空间里杂乱无章的数据经过降维工具之后获取到最本质的特征么。这正是特征抽取所要做的事情。

最后总结之：
1. 特征提取是从杂乱无章的世界中，去到更高层的世界去俯瞰原始世界，你会发现很多杂乱无章的物理现象中背后暗含的道理是想通的，这时候你想用一个更加普世的观点和理论去解释原先的理论，这个是特征提取要做的事情。
2. 而你仍呆在原始世界中，只是想对现有的“取其精华，去其糟粕”，这个是所谓特征选择。只是对现有进行筛选。
3. 特征提取和特征选择统称为降维。（Dimension Reduction）
